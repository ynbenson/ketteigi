# -*- coding: utf-8 -*-
"""Decision Tree Starter.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xnGKnQpy8WD7x51LR8L4cF8SaKMAQ-69

# Decision Tree Fitting Starter

An example of loading data, manually specifying a decision tree, predicting values for new features, and visualizing the resulting decision tree.

See below for project description.


* Class name: CS 460g
* Assignment: Homework #1 
* Student Name: [ADD YOUR NAME HERE]
* Student Number: [ADD YOUR ID NUMBER HERE]
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

def tree_predict(tree,feat_test):

  # predict the label and compute the score for each row in feat_test for a given tree
  
  ### fast way of initializing list in python, I guess 
  sco = np.empty((feat_test.shape[0],1))
  sco.fill(np.nan) # good practice to use NaNs to initialize
  
  # compute P(true|feat) at the leaf node for each test case
  for i, feat_cur in enumerate(feat_test):

    # run the current example down the tree
    node = tree
    while True:      
      if 'left' in node:
        # at a split node
        if feat_cur[node['dimension']] < node['threshold']:
          node = node['left']
        else:
          node = node['right']
      else:
        # at a leaf node
        sco[i] = node['freq'][1] / np.sum(node['freq']);
        break

  #print("-*-"*30)
  #print(np.sum(sco)/len(sco))
  label_pred = .5 <= sco;

  return label_pred, sco

"""# Make Data"""

def gen_from_mean_and_cov(means, covs, labels, mode='train', count=100):
  
  np.random.seed(0) if mode == 'train' else np.random.seed(1)
      
  vals = np.array([]).reshape(0,len(means[0])+1)

  for i, (mean,cov,label) in enumerate(zip(means,covs,labels)):
    vals_new = np.random.multivariate_normal(mean,cov,count);
    vals_new = np.hstack([vals_new,np.ones((vals_new.shape[0],1))*label])
    vals = np.vstack([vals,vals_new])    

  df = pd.DataFrame(data=vals,columns=['x1','x2','y'])

  return df

# There are 6 different simple training datasets

datasets = {
    'train1':gen_from_mean_and_cov([[10,1],[0,4]],[[[2, 0],[0, 2]],[[2, 0],[0, 2]]],[1,0]),
    'train2':gen_from_mean_and_cov([[10,1],[0,4]],[[[2, 0],[0, 2]],[[2, 0],[0, 2]]],[0,1]),
    'train3':gen_from_mean_and_cov([[0,1],[0,3.5]],[[[1, .8],[.8, 1]],[[1, .8],[.8, 1]]],[1,0]),
    'train4':gen_from_mean_and_cov([[1,0],[3,1]],[[[1, .8],[.8, 1]],[[1, .9],[.9, 1]]],[1,0]),
    'train5':gen_from_mean_and_cov([[0,0],[4,0],[-4,0],[0,4],[0,-4]],[[[1, 0],[0, 1]],[[1, 0],[0, 1]],[[1, 0],[0, 1]],[[1, 0],[0, 1]],[[1, 0],[0, 1]]],[1,0,0,0,0]),
    'train6':gen_from_mean_and_cov(
    [[0,0],[4,0],[-4,0],[0,4],[0,-4],[-4,4],[4,4],[4,-4],[-4,-4]],
    [[[1, 0],[0, 1]],[[1, 0],[0, 1]],[[1, 0],[0, 1]],[[1, 0],[0, 1]],[[1, 0],[0, 1]],[[1, 0],[0, 1]],[[1, 0],[0, 1]],[[1, 0],[0, 1]],[[1, 0],[0, 1]]],
    [1,0,0,0,0,1,1,1,1]),
    'test1':gen_from_mean_and_cov([[10,1],[0,4]],[[[2, 0],[0, 2]],[[2, 0],[0, 2]]],[1,0],mode='test'),
    'test2':gen_from_mean_and_cov([[10,1],[0,4]],[[[2, 0],[0, 2]],[[2, 0],[0, 2]]],[0,1],mode='test'),
    'test3':gen_from_mean_and_cov([[0,1],[0,3.5]],[[[1, .8],[.8, 1]],[[1, .8],[.8, 1]]],[1,0],mode='test'),
    'test4':gen_from_mean_and_cov([[1,0],[3,1]],[[[1, .8],[.8, 1]],[[1, .9],[.9, 1]]],[1,0],mode='test'),
    'test5':gen_from_mean_and_cov([[0,0],[4,0],[-4,0],[0,4],[0,-4]],[[[1, 0],[0, 1]],[[1, 0],[0, 1]],[[1, 0],[0, 1]],[[1, 0],[0, 1]],[[1, 0],[0, 1]]],[1,0,0,0,0],mode='test'),
    'test6':gen_from_mean_and_cov(
    [[0,0],[4,0],[-4,0],[0,4],[0,-4],[-4,4],[4,4],[4,-4],[-4,-4]],
    [[[1, 0],[0, 1]],[[1, 0],[0, 1]],[[1, 0],[0, 1]],[[1, 0],[0, 1]],[[1, 0],[0, 1]],[[1, 0],[0, 1]],[[1, 0],[0, 1]],[[1, 0],[0, 1]],[[1, 0],[0, 1]]],
    [1,0,0,0,0,1,1,1,1],mode='test')
}

"""# Visualize Data"""

def gini_impurity(node):
  node_total_element = len(node.values)
  
  # Sort the node by x1 and x2
  df1 = node.sort_values(by=['x1'], ascending=[True])
  df2 = node.sort_values(by=['x2'], ascending=[True])

  #   row_values = df1.index.values
  #   column_values = df1.columns.values
  #   split_value = df1.at[row_values[0], column_values[0]] 
  
  # Compute the midpoint (split points) for df1 and df2
  midpoint1 = []
  for i in range(len(df1.values)):
    if i == 0:
      continue
    else:
      midpoint1.append((df1.iloc[i]['x1']+df1.iloc[i-1]['x1'])/2)
   
  midpoint2 = []
  for i in range(len(df2.values)):
    if i == 0:
      continue
    else:
       midpoint2.append((df2.iloc[i]['x2']+df2.iloc[i-1]['x2'])/2)
        
  """
  print("midpoint1")
  print(midpoint1)
  
  print("midpoint2")
  print(midpoint2)
  """
  
  
  min_gini1 = 1
  min_gini2 = 1
  min_gini1_midpoint = 0
  for i in range(0, len(midpoint1)):

    total_y1_below_mid = 0
    total_y1_above_mid = 0
    num_element_below_mid = 0
    num_element_above_mid = 0
    for j in range(0, len(df1.values)):
      if df1.iloc[j]['x1'] < midpoint1[i]:
        total_y1_below_mid += df1.iloc[j]['y']
        num_element_below_mid += 1
      else: # df1[i]['x'] >=  midpoint1[i]
        total_y1_above_mid += df1.iloc[j]['y']
        num_element_above_mid += 1
        
    total_y0_below_mid = num_element_below_mid - total_y1_below_mid
    total_y0_above_mid = num_element_above_mid - total_y1_above_mid
        
    gini_imp1 = 1 - (total_y0_below_mid/num_element_below_mid)**2 - (total_y1_below_mid/num_element_below_mid)**2
    gini_imp2 = 1 - (total_y0_above_mid/num_element_above_mid)**2 - (total_y1_above_mid/num_element_above_mid)**2
    gini = (num_element_below_mid/node_total_element)*gini_imp1 + (num_element_above_mid/node_total_element)*gini_imp2
    
    if gini < min_gini1:
      min_gini1 = gini
      min_gini1_midpoint = midpoint1[i]
      
  min_gini2_midpoint = 0
  for i in range(len(midpoint2)):
    total_y1_below_mid = 0
    total_y1_above_mid = 0
    num_element_below_mid = 0
    num_element_above_mid = 0
    for j in range(len(df2.values)):
      if df2.iloc[j]['x2'] < midpoint2[i]:
        total_y1_below_mid += df2.iloc[j]['y']
        num_element_below_mid += 1
      else: # df1[i]['x'] >=  midpoint1[i]
        total_y1_above_mid += df2.iloc[j]['y']
        num_element_above_mid += 1
        
    total_y0_below_mid = num_element_below_mid - total_y1_below_mid
    total_y0_above_mid = num_element_above_mid - total_y1_above_mid
        
    gini_imp1 = 1 - (total_y0_below_mid/num_element_below_mid)**2 - (total_y1_below_mid/num_element_below_mid)**2
    gini_imp2 = 1 - (total_y0_above_mid/num_element_above_mid)**2 - (total_y1_above_mid/num_element_above_mid)**2
    gini = (num_element_below_mid/node_total_element)*gini_imp1 + (num_element_above_mid/node_total_element)*gini_imp2
    
    if gini < min_gini2:
      min_gini2 = gini
      min_gini2_midpoint = midpoint2[i]
      
      
  split_attr = 'x1' if min_gini1 < min_gini2 else 'x2'
  split_point = min_gini1_midpoint if min_gini1 < min_gini2 else min_gini2_midpoint
  min_gini = min_gini1 if min_gini1 < min_gini2 else min_gini2
  
  # print("split_attr:", split_attr, "\nsplit_point:",split_point, "\nmin_gini:",min_gini)
  
  return (split_attr, split_point, min_gini)

"""# Fit Decision Tree"""

#
# TODO write a function (or functions) to automatically construct a decision 
#      tree, store the result as the variable "tree"
#
# Requirements:
#   - Must be written in Python 3 using standard libraries.
#   - You may only use binary splits
#   - To determine a set of valid splits, sort each attribute separately and
#     compute the midpoint between consecutive values.
#   - You must use Gini or InfoGain as the node impurity measure.
#   - It only needs to work for binary binary labels {0,1}
#   - You can only use basic Python and Numpy functions to build the tree.  You 
#     may not use a pre-existing higher-level library for decision tree fitting.
#     However, you may find it useful to do that to compare how your algorithm
#     is working.
#
#   - Train a decision tree for all 6 datasets defined above: train1, train2,
#     train3, train4, train5, and train6.
#
#   - Compute the percent correct of predictions made by your decision tree on
#     the training set and the corresponding test set (don't merge the two
#     datasets, compute percent correct separately for each).  Include both
#     numbers, for each dataset, in a single table (for a total of 12 numbers).
#
# Source code:
#
#  - Submit all source code.  You may convert this into a set of Python scripts 
#    or write all code in an iPython notebook.
#
# Writeup:
#
#  - Submit a Word document or PDF (all other formats will not be graded!).
#  - Include your name, student number, class name, and "Homework #1".
#  - It must:
#    - Include the classifier visualization below for each dataset (overlay the
#      test set points)
#    - Include a description of any unique aspects of your solution.
#    - Include instructions for how to run your solution.
#
# Graduate Student Extra Work (Small Bonus for Undergraduates):
#
#  Choose one of the following:
#
#  - Implement Gini, InfoGain, and Misclassification error split criteria.
#    Compare the accuracy on the training set.  Which one is better?  Why?
#
#  - Implement the ability to limit the maximum depth of the decision tree.
#    What is the optimal depth for each dataset?  Why did you pick this value?
#
def train_tree(df):
  tree_left={}
  tree_right={}
  tree = {}
  
  # Compute gini impurity
  # Get the count of 0's and 1's for frequency => change gini_impurity function to do that 
  dm, tree['threshold'], gini_value = gini_impurity(df)
  
  
  # Set dimension based on which axis (x1 or x2) the split occurred in x1 dimension
  if dm=='x1':
    tree['dimension']=0
  else:
    tree['dimension']=1

  # Sort the tree 
  if tree['dimension']==1:
    df = df.sort_values(by=['x2'], ascending=[True])
  else:
    df = df.sort_values(by=['x1'], ascending=[True])


    
  # find a place where tree['threshold'] occurs 
  split_position = 0
  for i in range(len(df.values)):
    if df.iloc[i][tree['dimension']] < tree['threshold']:
      split_position += 1
      
  dfs0 = df[0:split_position]
  dfs1 = df[split_position:]
  tree_left = dfs0
  tree_right = dfs1

  #print("gini values : ", gini_value)

  quit_value = 0.15
  # if the leaves are purely classified
  if gini_value <= quit_value or (len(dfs0) < 3) or (len(dfs1) < 3):
    #Compute the frequency
    tree_left_zero_count = 0
    tree_left_one_count = 0
    tree_right_zero_count = 0
    tree_right_one_count = 0
    
    for val, cnt in tree_left.y.value_counts().iteritems():
      if val == 0:
        tree_left_zero_count = cnt
      else:
        tree_left_one_count = cnt

    for val, cnt in tree_right.y.value_counts().iteritems():
      if val == 0:
        tree_right_zero_count = cnt
      else:
        tree_right_one_count = cnt
    
    
    # print("tree_left_zero_count :", tree_left_zero_count, "\ntree_left_one_count:", tree_left_one_count)
    # print("tree_right_zero_count:", tree_right_zero_count, "\ntree_right_one_count:", tree_right_one_count)

#     if tree_left_zero_count < tree_left_one_count:
#       pc = pc + (tree_left_one_count - tree_left_zero_count)/(tree_left_zero_count + tree_left_one_count)
#     else:
#       pc = pc + (tree_left_zero_count - tree_left_one_count)/(tree_left_zero_count + tree_left_one_count)

#     if tree_right_zero_count < tree_right_one_count:
#       pc = pc + (tree_right_one_count - tree_right_zero_count)/(tree_right_zero_count + tree_right_one_count)
#     else:
#       pc = pc + (tree_right_zero_count - tree_right_one_count)/(tree_right_zero_count + tree_right_one_count)
    
 
    tree_left = {}
    tree_right = {}
    tree_left['freq'] = np.array([tree_left_zero_count, tree_left_one_count],dtype=np.float32)
    tree_right['freq'] = np.array([tree_right_zero_count, tree_right_one_count],dtype=np.float32)
    tree['left'] = tree_left
    tree['right']=tree_right
  
  # If the splitting can be performed further more
  else :

    tree['left'] = train_tree(tree_left)
    tree['right'] = train_tree(tree_right)
    
  return tree

"""# Visualize Classifier

For a range of values in feature space show the score, decision, decision boundary, and samples.
"""

def vis2d_classifier(tree,data):
  
  # create the domain for the plot
  x1_min = data.x1.min()
  x1_max = data.x1.max()
  x2_min = data.x2.min()
  x2_max = data.x2.max()

  x1 = np.linspace(x1_min, x1_max, 200)
  x2 = np.linspace(x2_min, x2_max, 200)
  X1,X2 = np.meshgrid(x1, x2)

  # convert it into a matrix (rows are locations, columns are features)
  vis_data = np.hstack([X1.reshape(-1,1),X2.reshape(-1,1)])

  # classify each location
  vis_class, vis_sco = tree_predict(tree,vis_data)
  
  #print("---"*20, "vis_sco:", "---"*20)
  #print(vis_sco)

  # convert back into image shapes
  vis_class = vis_class.reshape(X1.shape)
  vis_sco = vis_sco.reshape(X1.shape)

  #
  # Make the plots
  #

  # show the function value in the background
  cs = plt.imshow(vis_sco,
    extent=(x1_min,x1_max,x2_max,x2_min), # define limits of grid, note reversed y axis
    cmap=plt.cm.jet, vmin=0.,vmax=1.)
  plt.clim(0,1) # defines the value to assign the min/max color

  # draw the line on top
  levels = np.array([.5])
  cs_line = plt.contour(X1,X2,vis_sco,levels, colors='k')

  plt.scatter(data.x1,data.x2,c=data.y,edgecolors='w',cmap=plt.get_cmap('jet'))

  # add a color bar
  CB = plt.colorbar(cs)

  plt.show()
  
#vis2d_classifier(tree,df)

# training data (change the data name to desired one, like train1, train2, etc.)
train_data = datasets['train3']

# get testing data (change the data name to desired one, like test1, test2, etc.)
test_data = datasets['test3']


# train the tree
tree = train_tree(train_data)

# print("% correct for training data : ", (perc_correct/2)*100, "%", sep="") 

# try it with testing data
vis2d_classifier(tree,test_data)